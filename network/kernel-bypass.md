# 3.3.3 高并发网络处理：从 C10K 到 C10M

从前面内核网络优化的章节描述中，可以总结出 Linux 内核网络协议栈为基础应用方案存在几个问题：

- 应用程序和网络协议栈的交互过程中存在用户态和内核态的频繁切换，操作系统在执行此类操作时，会涉及当前进程上下文切换，TLB也会被频繁更新，导致 MMU 需要经常访问页表，这些都影响数据收发的时延。
- 用户空间缓存和内核空间缓存之间的复制行为，也消耗了大量的时间。
- 另外内核协议栈对数据的各种封包、解包也会消耗CPU 时钟。

以上几个缺点都会提高网络包的整体时延，特别是在云计算网络核心节点，这些环境通常有非常高的并发数据量，云架构中复杂的服务治理已经将C10K的问题发展到C10M（单机并发 1000 万）。一个大规模集群内，面对东西向规模性的Gbp/s数据流量，关键节点的挑战是用户态协议栈和多核并发问题。

这个时候，Linux 网络协议栈进行的各种优化策略，基本都没有太大效果。网络协议栈的冗长流程才是最主要的性能负担。

不过技术总在发展，业界专家已经想到了很多新的技术来解决上述问题，这些新技术包括 DPDK、RDMA、XDP等。

## DPDK

数据平面开发工具包 （data plane development kit， DPDK） 是在用户态运行的一组软件库和驱动程序，可在大部分主要的 CPU 体系结构上加速网络数据包的处理。作为 Linux 基金会下的开源项目， DPDK 在推动通用 CPU 在高性能网络环境中发挥了很大的作用。

### DPDK 技术概述

传统的网络设备（NIC），在执行底层数据平面功能，比如数据包的转发和路由，使用的都是专用的集成电路芯片(ASIC), 以 ASIC 芯片 + 配套软件的产品架构提供的吞吐量也能达到高性能网络的要求，但其新产品推出收到芯片开发周期的限制，而且 ASIC 供应商之间也不存在软件移植的可能。

后续 Intel、Cavium等半导体公司开始将通用多核处理器引入网络数据处理领域，进行底层的数据包处理，这些方案再处理器性能和成本方面可以和 ASIC 芯片的网络产品竞争，但问题在于，以 Linux 内核协议栈为基础的网络方案存在许多瓶颈，无法高性能地处理数据包。

此时需要一个解决方案来消除这些瓶颈，同时还要保持原有 Linux 程序的兼容性，新方案最好还要以库的形式打包到 Linux 发行版中，在用户需要时来管理各种网络设备。

这些目标随着 Intel 基于Nehalem微架构的 Xeon 处理器推出的 DPDK 而实现，DPDK 绕过（bypass） Linux 内核，在用户态执行数据包处理，以提供尽可能的网络性能。

DPDK 程序运行在操作系统的用户态，利用自带的数据平台库进行数据包的发送、处理和接收、绕过运行在内核态的网络协议栈，大幅提升数据包的处理效率。

DPDK 的网卡驱动程序运行在用户态，屏蔽了网卡硬件发起的大部分中断，采用主动轮询的方式持续检查网卡的接受/发送队列，查看是否有新数据到达或者是否可以继续发送数据，而从实现高吞吐量和低时延，因为 DPDK的驱动程序 也被称为 轮询模式驱动程序 (poll mode driver)。


<div  align="center">
	<img src="../assets/dpdk.png" width = "550"  align=center />
</div>

图：DPDK 与 传统内核网络的对比
- 左边是原来的方式数据从 网卡 -> 驱动 -> 协议栈 -> Socket接口 -> 业务
- 右边是DPDK的方式，基于UIO（Userspace I/O）旁路数据。数据从 网卡 -> DPDK轮询模式-> DPDK基础库 -> 业务


不过 Linux 内核 仍然是 DPDK 实现的基础，比如 内核中的 UIO 驱动框架，它为 DPDK 驱动程序 提供获取寄存器的地址、中断计数等功能， 另外 内核提供的大页 机制，也是 DPDK 进行内存管理的基础。

### DPDK 性能指标

以下为 DPVS (LVS的DPDK优化版本)与 传统 LVS 在 PPS 转发上的指标对比，性能提升约 300%；

<div  align="center">
	<img src="../assets/dpvs-performance.png" width = "550"  align=center />
</div>

